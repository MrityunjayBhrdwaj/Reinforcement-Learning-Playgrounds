<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<title>K-Arm Bandits - Learn Reinforcement Learning The fun way</title>
	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<!-- <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"> -->
	<meta name="viewport" content="width=1024">

	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicon/apple-touch-icon-114x114.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#311e3e">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#311e3e">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#311e3e">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">

</head>

<body>
  <div class="flex-container">
  <header class="main-header">
  <div class="wrapper">
    <div class="header-flex">
      <div class="menu-icon-container">
        <span class="menu-icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
      </div>
      <nav class="main-nav">
        <span class="menu-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
        <ul>

          <!-- TODO: BRAINSTORM: decide where to put these blog and about page link... i am thinking of in the footer but i also need to put some link on the header (or do i??) -->
          <!-- <li><a href="/">Blog</a></li>
          <li><a href="/about">About</a></li> -->

        
          
            <li><a href="/reinforcement/learning/Bandits.html" class="recent-item" ><span>K-Arm Bandits</span></a></li>
          
        
          
            <li><a href="/reinforcement/learning/monteCarlo.html" class="recent-item" ><span>Monte Carlo</span></a></li>
          
        
          
            <li><a href="/reinforcement/learning/dynProg.html" class="recent-item" ><span>Dynamic Programming</span></a></li>
          
        

        </ul>
      </nav>
      <p class="logo"><a href="/">Reinforcement Learning Playgrounds</a></p>
      <div class="search-icon-container">
        <span class="search-icon"><a><i class="fa fa-search" aria-hidden="true"></i></a></span>
      </div>
    </div>
  </div>


  <!-- including plotlyjs -->
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

</header> <!-- End Header -->


  <!-- including mathjax -->
    <script async type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: [
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js",
        "a11y/accessibility-menu.js"
      ],
      jax: ["input/TeX", "output/CommonHTML"],
      TeX: {
        extensions: [
          "AMSmath.js",
          "AMSsymbols.js",
          "noErrors.js",
          "noUndefined.js",
        ]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Import all the required dependencies -->
  

  

  

  

  

  

  

  <!-- From the web -->
  


  <!-- imported the required visualization script for cover-visualization -->
  <article class="article-page">
    <div class="page-image" style='height: 1100px'>
      <embed id="myFrame" style="background-size: cover;padding: 0%;" frameborder=0 class="cover-image" width="90%" align="middle" padding="0" scrolling="no" src='/assets/viz/bandits/index.html'></embed>
    </div>
    <div class="wrapper">
      <div class="page-content">
        <div class="header-page">
          <h1 class="page-title">K-Arm Bandits</h1>


          <!-- <div class="page-date"><time datetime="2020-07-24 00:00:00 +0530">2020, Jul 24</time></div> -->
        </div>
        <ul>
  <li><strong>Nonassociative setting</strong>: one that does not involve learning to act in more than one situation</li>
</ul>

<script type="math/tex; mode=display">\require{\infty}</script>

<h2 id="k-armed-bandit-problem">K-armed Bandit Problem</h2>

<blockquote>
  <p>In K-armed Bandit Problem, You are faced repeatedly with a choice among K-options, or actions. After each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. your objective is to maximize the expected total reward over some time, for example, over 1000 action selections, or time steps.</p>
</blockquote>

<ul>
  <li>
    <p><strong>Value</strong> of that action is the expected reward given that that action is selected</p>

    <script type="math/tex; mode=display">q_{*}(a) \doteq \mathbb{E}\left[R_{t} | A_{t}=a\right]</script>
  </li>
  <li>
    <p>In practice, since we don’t know that exact action value we tend to use the estimated value of action ‘a’ at timestep t as <script type="math/tex">Q_t(a)</script>. And one of our goals is to be as close to <script type="math/tex">q*(a)</script> as possible….</p>
  </li>
  <li>
    <p>Although the agent that only exploit by choosing greedily might perform better than the agent who explores in the initial few steps, it turns out that in the long run, the one who explores performs better because it was able to assess the action that might prove to be much better in the long run whereas the greedy method often gets stuck performing the suboptimal actions… so it’s almost always better to explore.</p>
  </li>
</ul>

<h2 id="action-value-methods">Action-value Methods</h2>

<ul>
  <li>
    <p>The methods for estimating the values of actions for using the estimates to make action selection decisions are collectively known as <strong>action-value methods</strong>…</p>
  </li>
  <li>
    <p>The simplest way of estimating the action values is to just take the sample average:</p>

    <script type="math/tex; mode=display">Q_{t}(a) \doteq \frac{\text { sum of rewards when } a \text { taken prior to } t}{\text { number of times } a \text { taken prior to } t}=\frac{\sum_{i=1}^{t-1} R_{i} \cdot \mathbb{1}_{A_{i}=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_{i}=a}}</script>

    <p>Plus, the simplest method of selecting the action is to always act greedily i.e:</p>

    <script type="math/tex; mode=display">A_t = \operatorname{argmax}_{a} Q_t(a)</script>
  </li>
</ul>

<h2 id="10-arm-test-bed">10-arm Test Bed</h2>

<ul>
  <li>
    <p>A better way of action selection would be to behave greedily most of the time, but every once in a while (with a small probability <script type="math/tex">\epsilon</script>, instead select randomly which is the exact reasoning behind the <script type="math/tex">\epsilon</script>-greedy algorithm…</p>

    <p>The advantage of this method is that in the limit, the action value converges to q*(a) for each action because until then it’s been visited infinite amount of time</p>

    <p>This also means that the probability of selecting the optimal action converges to greater than <script type="math/tex">1-\epsilon</script></p>
  </li>
  <li>
    <p>If the reward variance is zero, then the greedy method would know the true value of each action after trying it once. In this case, the greedy method might actually perform best but even in the deterministic case there is still a large advantage to explore because there could be the case where the action value is deterministic but the environment is non-stationary which means it is possible that one of the non-optimal action become optimal over time…</p>
  </li>
  <li>
    <p>So, our original formulation is not suited for incremental learning that we have in the RL setting but with slight modification (see derivation 2.3) we can create our first simple bandit algorithm:-</p>
  </li>
</ul>

<div style="margin: 0 auto; text-align: center">
    <img src="/assets/img/posts_imgs/bandits/body/banditAlgo.png" width="80%" />
</div>

<h2 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h2>

<ul>
  <li>
    <p>In non-stationary problem it always makes much more sense to give more weightage to the more-recent rewards rather than initial ones which leads to our second method for estimating action values:-</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
  Q_{n+1} &=Q_{n}+\alpha\left[R_{n}-Q_{n}\right] \\
  &=\alpha R_{n}+(1-\alpha) Q_{n} \\
  &=\alpha R_{n}+(1-\alpha)\left[\alpha R_{n-1}+(1-\alpha) Q_{n-1}\right] \\
  &=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} Q_{n-1} \\
  &=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} \alpha R_{n-2}+\\
  & \cdots+(1-\alpha)^{n-1} \alpha R_{1}+(1-\alpha)^{n} Q_{1} \\
  &=(1-\alpha)^{n} Q_{1}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} R_{i}
  \end{aligned} %]]></script>

    <p>This method is also known as <strong>exponential recency-weighted average</strong></p>
  </li>
  <li>
    <p>According to stochastic approximation theory we an assure convergence of a method if it satisfies these 2 conditions:-</p>

    <script type="math/tex; mode=display">% <![CDATA[
\sum_{n=1}^{\infty}\alpha_n (a) = \infty \hspace{25pt} \text{and} \hspace{25pt} \sum_{n=1}^{\infty}\alpha^2_n (a) < \infty %]]></script>

    <p>The first condition guarantees that the steps are large enough to eventually overcome any initial conditions or random fluctuations.</p>

    <p>The second condition guarantees that eventually the steps become small enough to assure convergence.</p>
  </li>
  <li>
    <p>Although the sample average obeys both the conditions but the constant step size parameter doesn’t obey the second one, which means it keeps updating the estimates in response to the most recently received rewards which is desirable in non-stationary cases…</p>
  </li>
</ul>

<h2 id="optimistic-initial-values">Optimistic Initial Values</h2>

<ul>
  <li>The optimistic greedy encourages action-value methods to explore because we start with a highly optimistic initial estimate the agent has to explore after getting disappointed by observing the new reward that is always smaller than the initial estimates…</li>
</ul>

<div style="margin: 0 auto; text-align: center">
    <img src="http://localhost:4000/assets/img/posts_imgs/bandits/body/optimisticGreedy.png" width="80%" />
</div>

<h2 id="upper-confidence-bound-action-selection-ucb">Upper Confidence Bound Action selection (UCB)</h2>

<ul>
  <li>The idea of <strong>upper confidence bound(UCB) action selection</strong> is that the square-root term is a measure of the uncertainty or variance in the estimate of <script type="math/tex">a</script>’s value. The quantity being maxed over is thus a sort of upper bound on the possible true value of action <script type="math/tex">a</script>, with <script type="math/tex">c</script> determining the confidence level. Each time <script type="math/tex">a</script> is selected the uncertainty is presumably reduced: <script type="math/tex">Nt(a)</script> increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than <script type="math/tex">a</script> is selected,<script type="math/tex">t</script> increases but <script type="math/tex">Nt(a)</script> does not, because it appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</li>
</ul>

<script type="math/tex; mode=display">A_t = \operatorname{argmax}_{a}{\left [ Q_t(a)  + c \sqrt{\frac{\ln t}{N_t(a)}} \quad \right ]}</script>

<div style="margin: 0 auto; text-align: center">
    <img src="http://localhost:4000/assets/img/posts_imgs/bandits/body/UCB.png" width="80%" />
</div>

<h2 id="gradient-bandit-algorithms">Gradient Bandit Algorithms</h2>

<ul>
  <li>
    <p>In gradient Bandit Algorithm we consider learning a numerical <strong>preference</strong> for each action <script type="math/tex">a</script>, which we denote <script type="math/tex">H_t(a) \in \mathbb{R}</script>. The larger the preference, the more often that action is taken.</p>

    <script type="math/tex; mode=display">\operatorname{Pr}\{A_t=a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} = \pi_t(a)</script>

    <p>here, <script type="math/tex">\pi_t(a)</script> is the probability of taking action <script type="math/tex">a</script> at time <script type="math/tex">t</script> also, known as policy ( which we will see in a later chapter)</p>

    <p>we can naturally learn the action preferences using the stochastic gradient ascent:-</p>

    <script type="math/tex; mode=display">H_{t+1}(a) = H_t (a) - \alpha(R_t -\bar{R}_t)\pi_t(a), \hspace{50px} \text{for all } a \neq A_t</script>
  </li>
  <li>
    <p>because we have the reward baseline term <script type="math/tex">\bar{R}_t</script>, even if we shift up the mean of the reward distribution the gradient bandit algorithm will adapt immediately unlike standard bandit algorithm.</p>
  </li>
  <li>
    <p>if we let the actions of our bandits affect only the immediate reward then the resulting task becomes an <strong>associative search task</strong> (contextual bandits) because here, the actions are associated with certain situations and we need to search for the best possible configuration (policy).</p>
  </li>
</ul>

<h2 id="associative-search-contextural-bandits">Associative Search (Contextural Bandits)</h2>

<ul>
  <li>contextual bandits are intermediate between k-armed bandit problem and the full RL problem, if the actions are allowed to affect the next situation as well as the reward then we have the full reinforcement learning problem.</li>
</ul>

        <div class="page-footer">
          <div class="page-tag">
            <span>Tags:</span>
            
            <a href="/tags#ML" class="tag">| ML</a>
            
            <a href="/tags#AI" class="tag">| AI</a>
            
            <a href="/tags#Reinforcement Learning" class="tag">| Reinforcement Learning</a>
            
            <a href="/tags#RL" class="tag">| RL</a>
            
            <a href="/tags#Bandits" class="tag">| Bandits</a>
            
            <a href="/tags#Arm" class="tag">| Arm</a>
            
            <a href="/tags#Multi-arm" class="tag">| Multi-arm</a>
            
          </div><!-- End Tags -->
          <div class="page-share">
            <span>Share:</span>
            <a href="https://twitter.com/intent/tweet?text=K-Arm Bandits&url=http://localhost:4000/reinforcement/learning/Bandits.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a href="https://facebook.com/sharer.php?u=http://localhost:4000/reinforcement/learning/Bandits.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a href="https://plus.google.com/share?url=http://localhost:4000/reinforcement/learning/Bandits.html" title="Share on Google+" rel="nofollow" target="_blank"><i class="fa fa-google" aria-hidden="true"></i></a>
          </div><!-- End Share -->
        </div>
        <!-- Removed disqus, newsletter, recent posts -->
        <div class="post-nav">
        <div>
          
          <a href="/reinforcement/learning/monteCarlo.html">&laquo;&nbsp;Monte Carlo</a>
          
        </div>
        <div class="post-nav-next">
          
        </div>
      </div>


      </div>
    </div> <!-- End Wrapper -->
  </article>
  <div class="search-box">
  <div class="wrapper">
    <div class="search-grid">
      <form class="search-form">
        <div id="search-container">
          <input type="text" id="search-input" class="search" placeholder="Search">
        </div>
      </form>
      <ul id="results-container" class="results-search"></ul>
      <div class="icon-close-container">
        <span class="search-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
      </div>
    </div>
  </div>
</div>

  <footer class="main-footer">
  <div class="copyright">
    <p>2021 &copy; <a href="http://localhost:4000"> Reinforcement Learning Playgrounds </a></p>
  </div>
</footer> <!-- End Footer -->

</div>

  <!-- JS -->
<script src="/assets/js/jquery-3.2.1.min.js"></script>
<script src="/assets/js/jekyll-search.js"></script>
<script>
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/search.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    fuzzy: false,
    exclude: ['Welcome']
  });
</script>
<script src="/assets/js/main.js"></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
