---
layout: post 
title : K-Arm Bandits
tags  : [ML,AI, Reinforcement Learning, RL, Bandits, Arm, Multi-arm]
title-seprator: "|"
categories: Reinforcement Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
height: 1100px
img: /posts_imgs/bandits/teaser/banditsScrst.png
viz: bandits/index.html

---
## Um.. What am i looking at?

The Idea Behind this visualization is to give an entire picture of Bandit Algorithm. where we can see all the components, from distribution of each slot machine to Current Expected Rewards.

At an expense of being a bit more verbose, here, we have 6 panels
1. **Code**: Shows the psudo code of Simple bandit algorithm.
2. **Bandits**: A visualization of Slot machines. here, instead of 10 arms we have split it into 10 different slot machines for visual clearity.
3. **Environments**: Since, our Environments are these 10 different slot machines, we have plot the reward distribution as well as mean of each distribution. The Idea that we will react the optimal policy when we estimate the mean of all the distribution as close as possible.
4. **Current Expected Reward**: As the name suggest, here we plot the expected rewards for all the slot machines at each time step.
5. **Relative Frequency**: Relative frequency of selecting each slot machine.
6. **Current Episode**: Plot of current reward sampled from the distribution of current selected slot machine, at each time step.

## What is a K-armed Bandit Problem?

> In K-armed Bandit Problem, You are faced repeatedly with a choice among K-options, or actions. After each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. your objective is to maximize the expected total reward over some time, for example, over 1000 action selections, or time steps.

* **Value** of that action is the expected reward given that that action is selected 

    $$
    q_{*}(a) \doteq \mathbb{E}\left[R_{t} | A_{t}=a\right]
    $$

* In practice, since we don’t know that exact action value we tend to use the estimated value of action ‘a’ at timestep t as $$Q_t(a)$$. And one of our goals is to be as close to $$q*(a)$$ as possible….

* Although the agent that only exploit by choosing greedily might perform better than the agent who explores in the initial few steps, it turns out that in the long run, the one who explores performs better because it was able to assess the action that might prove to be much better in the long run whereas the greedy method often gets stuck performing the suboptimal actions… so it's almost always better to explore.


### Action-value Methods
 
* The methods for estimating the values of actions for using the estimates to make action selection decisions are collectively known as **action-value methods**…

* The simplest way of estimating the action values is to just take the sample average:

    $$
    Q_{t}(a) \doteq \frac{\text { sum of rewards when } a \text { taken prior to } t}{\text { number of times } a \text { taken prior to } t}=\frac{\sum_{i=1}^{t-1} R_{i} \cdot \mathbb{1}_{A_{i}=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_{i}=a}}
    $$


    Plus, the simplest method of selecting the action is to always act greedily i.e:

    $$A_t = \operatorname{argmax}_{a} Q_t(a)$$

### 10-arm Test Bed

* A better way of action selection would be to behave greedily most of the time, but every once in a while (with a small probability $$\epsilon$$, instead select randomly which is the exact reasoning behind the $$\epsilon$$-greedy algorithm… 

    The advantage of this method is that in the limit, the action value converges to q*(a) for each action because until then it's been visited infinite amount of time

    This also means that the probability of selecting the optimal action converges to greater than $$1-\epsilon$$


* If the reward variance is zero, then the greedy method would know the true value of each action after trying it once. In this case, the greedy method might actually perform best but even in the deterministic case there is still a large advantage to explore because there could be the case where the action value is deterministic but the environment is non-stationary which means it is possible that one of the non-optimal action become optimal over time…
  
* So, our original formulation is not suited for incremental learning that we have in the RL setting but with slight modification (see derivation 2.3) we can create our first simple bandit algorithm:-


<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/bandits/body/banditAlgo.png"  width="80%" />
</div>

##Further Reading
* https://en.wikipedia.org/wiki/Multi-armed_bandit
* https://www.ualberta.ca/computing-science/graduate-studies/course-directory/courses/bandit-algorithms.html
