---
layout: post 
title : K-Arm Bandits
tags  : [ML,AI, Reinforcement Learning, RL, Bandits, Arm, Multi-arm]
title-seprator: "|"
categories: Reinforcement Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
height: 1100px
img: /posts_imgs/bandits/teaser/banditsScrst.png
viz: bandits/index.html

---

* **Nonassociative setting**: one that does not involve learning to act in more than one situation

$$
\require{\infty}
$$

## K-armed Bandit Problem

> In K-armed Bandit Problem, You are faced repeatedly with a choice among K-options, or actions. After each choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. your objective is to maximize the expected total reward over some time, for example, over 1000 action selections, or time steps.

* **Value** of that action is the expected reward given that that action is selected 

    $$
    q_{*}(a) \doteq \mathbb{E}\left[R_{t} | A_{t}=a\right]
    $$

* In practice, since we don’t know that exact action value we tend to use the estimated value of action ‘a’ at timestep t as $$Q_t(a)$$. And one of our goals is to be as close to $$q*(a)$$ as possible….

* Although the agent that only exploit by choosing greedily might perform better than the agent who explores in the initial few steps, it turns out that in the long run, the one who explores performs better because it was able to assess the action that might prove to be much better in the long run whereas the greedy method often gets stuck performing the suboptimal actions… so it's almost always better to explore.

## Action-value Methods
 
* The methods for estimating the values of actions for using the estimates to make action selection decisions are collectively known as **action-value methods**…

* The simplest way of estimating the action values is to just take the sample average:

    $$
    Q_{t}(a) \doteq \frac{\text { sum of rewards when } a \text { taken prior to } t}{\text { number of times } a \text { taken prior to } t}=\frac{\sum_{i=1}^{t-1} R_{i} \cdot \mathbb{1}_{A_{i}=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_{i}=a}}
    $$


    Plus, the simplest method of selecting the action is to always act greedily i.e:

    $$A_t = \operatorname{argmax}_{a} Q_t(a)$$

## 10-arm Test Bed

* A better way of action selection would be to behave greedily most of the time, but every once in a while (with a small probability $$\epsilon$$, instead select randomly which is the exact reasoning behind the $$\epsilon$$-greedy algorithm… 

    The advantage of this method is that in the limit, the action value converges to q*(a) for each action because until then it's been visited infinite amount of time

    This also means that the probability of selecting the optimal action converges to greater than $$1-\epsilon$$


* If the reward variance is zero, then the greedy method would know the true value of each action after trying it once. In this case, the greedy method might actually perform best but even in the deterministic case there is still a large advantage to explore because there could be the case where the action value is deterministic but the environment is non-stationary which means it is possible that one of the non-optimal action become optimal over time…
  
* So, our original formulation is not suited for incremental learning that we have in the RL setting but with slight modification (see derivation 2.3) we can create our first simple bandit algorithm:-


<div style="margin: 0 auto; text-align: center">
    <img src="{{site.baseurl}}/assets/img/posts_imgs/bandits/body/banditAlgo.png"  width="80%" />
</div>

## Tracking a Nonstationary Problem

* In non-stationary problem it always makes much more sense to give more weightage to the more-recent rewards rather than initial ones which leads to our second method for estimating action values:-

    $$
    \begin{aligned}
    Q_{n+1} &=Q_{n}+\alpha\left[R_{n}-Q_{n}\right] \\
    &=\alpha R_{n}+(1-\alpha) Q_{n} \\
    &=\alpha R_{n}+(1-\alpha)\left[\alpha R_{n-1}+(1-\alpha) Q_{n-1}\right] \\
    &=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} Q_{n-1} \\
    &=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} \alpha R_{n-2}+\\
    & \cdots+(1-\alpha)^{n-1} \alpha R_{1}+(1-\alpha)^{n} Q_{1} \\
    &=(1-\alpha)^{n} Q_{1}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} R_{i}
    \end{aligned}
    $$

    This method is also known as **exponential recency-weighted average**

* According to stochastic approximation theory we an assure convergence of a method if it satisfies these 2 conditions:-


    $$ \sum_{n=1}^{\infty}\alpha_n (a) = \infty \hspace{25pt} \text{and} \hspace{25pt} \sum_{n=1}^{\infty}\alpha^2_n (a) < \infty $$

    The first condition guarantees that the steps are large enough to eventually overcome any initial conditions or random fluctuations.

    The second condition guarantees that eventually the steps become small enough to assure convergence.


* Although the sample average obeys both the conditions but the constant step size parameter doesn’t obey the second one, which means it keeps updating the estimates in response to the most recently received rewards which is desirable in non-stationary cases…

## Optimistic Initial Values

* The optimistic greedy encourages action-value methods to explore because we start with a highly optimistic initial estimate the agent has to explore after getting disappointed by observing the new reward that is always smaller than the initial estimates… 

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/bandits/body/optimisticGreedy.png"  width="80%" />
</div>


## Upper Confidence Bound Action selection (UCB)

* The idea of **upper confidence bound(UCB) action selection** is that the square-root term is a measure of the uncertainty or variance in the estimate of $$a$$’s value. The quantity being maxed over is thus a sort of upper bound on the possible true value of action $$a$$, with $$c$$ determining the confidence level. Each time $$a$$ is selected the uncertainty is presumably reduced: $$Nt(a)$$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than $$a$$ is selected,$$t$$ increases but $$Nt(a)$$ does not, because it appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.

$$A_t = \operatorname{argmax}_{a}{\left [ Q_t(a)  + c \sqrt{\frac{\ln t}{N_t(a)}} \quad \right ]} $$

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/bandits/body/UCB.png"  width="80%" />
</div>


## Gradient Bandit Algorithms

* In gradient Bandit Algorithm we consider learning a numerical **preference** for each action $$a$$, which we denote $$H_t(a) \in \mathbb{R}$$. The larger the preference, the more often that action is taken.

    $$
    \operatorname{Pr}\{A_t=a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} = \pi_t(a)
    $$

    here, $$\pi_t(a)$$ is the probability of taking action $$a$$ at time $$t$$ also, known as policy ( which we will see in a later chapter)

    we can naturally learn the action preferences using the stochastic gradient ascent:-

    $$
    H_{t+1}(a) = H_t (a) - \alpha(R_t -\bar{R}_t)\pi_t(a), \hspace{50px} \text{for all } a \neq A_t
    $$

* because we have the reward baseline term $$\bar{R}_t$$, even if we shift up the mean of the reward distribution the gradient bandit algorithm will adapt immediately unlike standard bandit algorithm.

* if we let the actions of our bandits affect only the immediate reward then the resulting task becomes an **associative search task** (contextual bandits) because here, the actions are associated with certain situations and we need to search for the best possible configuration (policy). 

## Associative Search (Contextural Bandits)

* contextual bandits are intermediate between k-armed bandit problem and the full RL problem, if the actions are allowed to affect the next situation as well as the reward then we have the full reinforcement learning problem.