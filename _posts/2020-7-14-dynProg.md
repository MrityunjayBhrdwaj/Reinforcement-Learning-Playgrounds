---
layout: post 
title : Dynamic Programming
tags  : [ML,AI, Reinforcement Learning, RL, Dynamic Programming, GridWorld]
title-seprator: "|"
categories: Reinforcement Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
height: 800px
img: /posts_imgs/dynProg/teaser/dynProgScrst.png
viz: dynProg/index.html
---

<br>

* **Dynamic programming** (DP) is a collection of algorithms that can be used to compute optimal policies given a *perfect model of the environment* as Markov Decision Process (MDP).

* all of the methods for calculating the optimal policies can be viewed as attempts to achieve much the same effect as DP only with less computation and without the assumption of a perfect model of the environment.

## Policy Evaluation (Prediction)

* The process of calculating the state-value function $$v_\pi$$ for an arbitrary policy $$\pi$$ is called **policy evaluation**.

$$
v_\pi(s) = \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right] = \sum_{a} \pi(a|s) \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_\pi (s')]}
$$

* the existence and uniqueness of $$v_\pi$$ are guaranteed as long as either $$\gamma < 1$$ or eventual termination is guaranteed from all states under the policy $$\pi$$

* The process of iteratively evaluating the value function from the initial approximation, $$v_0$$ is known as **iterative policy evaluation** inwhich each successive approximation is obtained by using the bellman equation for $$v_\pi$$ as an update rule:
  

    $$
    v_{k+1}(s) =\sum_{a} \pi(a|s) \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_k (s')]}
    $$

    In general, the sequence $$v_0,v_1,..,$$ of approximate value functions converges to $$v_\pi$$ as $$k \rightarrow \infty$$, here, we can think of a $$v_\pi$$ as some fixed point.

* In DP all the updates are called **expected updates** because they are based on an expectation over all possible next states rather than on a sample next state.

* The order in which states have their values updated during the sweep has a *significant influence on the rate of convergence*.

* Full algorithm of iterative policy evaluation

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/iterativePolicyEvalAlgo.png"  width="800px" />
</div>


## Policy Improvement

* The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called **policy improvement**.

    Suppose the new greedy policy, $$\pi'$$  is as good as, but not better than the older policy $$\pi$$. then $$v_\pi = v_\pi'$$, and from it follow that for all $$s \in \mathcal{S}$$:
    $$
    v_{\pi'}(s) =\max_a \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_{\pi'} (s')]}
    $$



* one way of assessing whether our current policy can be improved is by considering a setting in which we select a specific action $$a$$ in-state $$s$$ and thereafter following the existing policy $$\pi$$. now, if we observe an improvement in expected return by selecting the action $$a$$ every time we are in state $$s$$  then this new policy would in fact be a better one overall. 

    we can formalize this idea by considering any pair of deterministic policies such that, for all $$s \in \mathcal{S}$$ in which :

    $$q_\pi(s, \pi'(s)) \geq v_\pi(s)$$

    then the **policy improvement theorem** states that the policy $$\pi'$$ must be as good as, or better than , $$\pi$$. that is it must obtain greater or equal expected return from all states $$s \in \mathcal{S}$$ i.e, $$v_{\pi'} \geq v_\pi(s)$$.


## Policy Iteration

* if we combine both the policy evaluation and improvement then we can obtain a sequence of monotonically improving policies and value functions and eventually converge to an optimal policy, this iterative process is known as **policy iteration**:-

    $$
    \pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{0}} \stackrel{\mathrm{I}}{\longrightarrow} \pi_{1} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{1}} \stackrel{\mathrm{I}}{\longrightarrow} \pi_{2} \stackrel{\mathrm{E}}{\longrightarrow} \cdots \stackrel{\mathrm{I}}{\longrightarrow} \pi_{*} \stackrel{\mathrm{E}}{\longrightarrow} v_{*}
    $$

    where $$\stackrel{\mathrm{E}}{\longrightarrow}$$ denotes a policy evaluation and $$\stackrel{\mathrm{I}}{\longrightarrow}$$ denotes a policy improvement. 

<!-- Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and the optimal value function in a finite number of iterations. -->



<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/PolicyIterAlgo.png"  width="800px" />
</div>

## Value Iteration

* if we stop the policy evaluation loop after just one sweep and combine it with the policy improvement what we endup with, is the **value iteration method**. it can be written as a simple update operation that combines both the operations stated earlier:-

$$
\begin{aligned}
v_{k+1}(s) & \doteq \max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{k}\left(S_{t+1}\right) | S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]
\end{aligned}
$$


<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/ValueIterAlgo.png"  width="800px" />
</div>

## Asynchronous Dynamic Programming

* Async algorithms update the values of states *in any order whatsoever, using whatever values of other states happen to be available*.

    * avoiding sweeps doesn't mean less computation, it just means that now, because we are not bounded by any specific way of selecting a state to update we can now put more emphasis on those states which leads to the optimal solution and update them more frequently and put less emphasis on regions of the environment which are not that important
    * an extreme case of the above assertion is that we can run an iterative DP algorithm at the same time that an agent is "actually" experiencing the MDP.i.e, we can update those states which we are currently visiting.

    * 


## Generalized Policy Iteration


* We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy-evaluation and improvement process interact, independent of the granularity and other details of the 2 processes.

<div style="margin: 0 auto; text-align: center">
    <span >
        <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/coordinateAscent.png"  width="300px" />
    </span>
    <span >
        <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/policyIterLoop.png"  width="200px" />
    </span>
</div>


* now, if we keep running both the processes then in theory, they stabilize only when a policy has been found that is greedy with respect to its own evaluation function i.e, we can't improve it further, this implies that the Bellman optimality equation holds and thus that policy and the value function are optimal.
* We can also think of these 2 steps as optimizing for 2 different goals but if we combine both. then, the ultimate goal is to reach $$v_*$$ and $$\pi_*$$ that's where these 2 processes converge eventually.--