---
layout: post 
title : Dynamic Programming
tags  : [ML,AI, Reinforcement Learning, RL, Dynamic Programming, GridWorld]
title-seprator: "|"
categories: Reinforcement Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
height: 800px
img: /posts_imgs/dynProg/teaser/dynProgScrst.png
viz: dynProg/index.html
---

<br>

## Um... what am i looking at?

This is a Demostration of Using Dynamic Programming Method in Grid World Environment.

Here, each arrows indicates where we should move from each grid position. On the right, we have visualized the internal graph of this grid world which shows how are we moving in the grid world.

## What is Dynamic Programming? 

* **Dynamic programming** (DP) is a collection of algorithms that can be used to compute optimal policies given a *perfect model of the environment* as Markov Decision Process (MDP).

* all of the methods for calculating the optimal policies can be viewed as attempts to achieve much the same effect as DP only with less computation and without the assumption of a perfect model of the environment.

### Policy Evaluation (Prediction)

* The process of calculating the state-value function $$v_\pi$$ for an arbitrary policy $$\pi$$ is called **policy evaluation**.

$$
v_\pi(s) = \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right] = \sum_{a} \pi(a|s) \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_\pi (s')]}
$$

* the existence and uniqueness of $$v_\pi$$ are guaranteed as long as either $$\gamma < 1$$ or eventual termination is guaranteed from all states under the policy $$\pi$$

* The process of iteratively evaluating the value function from the initial approximation, $$v_0$$ is known as **iterative policy evaluation** inwhich each successive approximation is obtained by using the bellman equation for $$v_\pi$$ as an update rule:
  

    $$
    v_{k+1}(s) =\sum_{a} \pi(a|s) \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_k (s')]}
    $$

    In general, the sequence $$v_0,v_1,..,$$ of approximate value functions converges to $$v_\pi$$ as $$k \rightarrow \infty$$, here, we can think of a $$v_\pi$$ as some fixed point.

* In DP all the updates are called **expected updates** because they are based on an expectation over all possible next states rather than on a sample next state.

* The order in which states have their values updated during the sweep has a *significant influence on the rate of convergence*.

* Full algorithm of iterative policy evaluation

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/iterativePolicyEvalAlgo.png"  width="800px" />
</div>


### Policy Improvement

* The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called **policy improvement**.

    Suppose the new greedy policy, $$\pi'$$  is as good as, but not better than the older policy $$\pi$$. then $$v_\pi = v_\pi'$$, and from it follow that for all $$s \in \mathcal{S}$$:
    $$
    v_{\pi'}(s) =\max_a \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_{\pi'} (s')]}
    $$



* one way of assessing whether our current policy can be improved is by considering a setting in which we select a specific action $$a$$ in-state $$s$$ and thereafter following the existing policy $$\pi$$. now, if we observe an improvement in expected return by selecting the action $$a$$ every time we are in state $$s$$  then this new policy would in fact be a better one overall. 

    we can formalize this idea by considering any pair of deterministic policies such that, for all $$s \in \mathcal{S}$$ in which :

    $$q_\pi(s, \pi'(s)) \geq v_\pi(s)$$

    then the **policy improvement theorem** states that the policy $$\pi'$$ must be as good as, or better than , $$\pi$$. that is it must obtain greater or equal expected return from all states $$s \in \mathcal{S}$$ i.e, $$v_{\pi'} \geq v_\pi(s)$$.


### Policy Iteration

* if we combine both the policy evaluation and improvement then we can obtain a sequence of monotonically improving policies and value functions and eventually converge to an optimal policy, this iterative process is known as **policy iteration**:-

    $$
    \pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{0}} \stackrel{\mathrm{I}}{\longrightarrow} \pi_{1} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{1}} \stackrel{\mathrm{I}}{\longrightarrow} \pi_{2} \stackrel{\mathrm{E}}{\longrightarrow} \cdots \stackrel{\mathrm{I}}{\longrightarrow} \pi_{*} \stackrel{\mathrm{E}}{\longrightarrow} v_{*}
    $$

    where $$\stackrel{\mathrm{E}}{\longrightarrow}$$ denotes a policy evaluation and $$\stackrel{\mathrm{I}}{\longrightarrow}$$ denotes a policy improvement. 

<!-- Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and the optimal value function in a finite number of iterations. -->



<div style="margin: 0 auto; text-align: center">
    <img src="{{site.url}}/assets/img/posts_imgs/dynProg/body/PolicyIterAlgo.png"  width="800px" />
</div>

## Further Reading
* Sutton and Butao ([link](http://incompleteideas.net/book/the-book-2nd.html))
* Deepmind Course on Reinforcement Learning ([link](https://www.youtube.com/watch?v=hMbxmRyDw5M))
